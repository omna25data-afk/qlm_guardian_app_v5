---
title: وضعية التفاعل (شرح الملف)
description: >
  تعلم كيفية تحقيق التوازن بين قدرات الـ LLM والكود التقليدي، وتنفيذ حواجز الحماية لإدارة سلوك الذكاء الاصطناعي غير القابل للتنبؤ.
---

يُعتبر التعامل مع نموذج اللغة الكبير (LLM) بمثابة استدعاء دالة برمجية (Function call) خطأ شائعاً، فالدوال التقليدية تتوقع منها نتائج حتمية، بينما نماذج اللغات تعتمد على الاحتمالات، وتكون غير قابلة للتنبؤ الدقيق وربما تعطي نتائج خاطئة أحياناً. لذا، يجب أن تعامل الـ LLM وكأنه مستخدم، وتقوم ببناء حواجز حماية (Guardrails) لاعتماد مدخلاته والتحقق منها.

## فصل المسؤوليات (Separation of concerns)

الـ LLMs مبدعة في مهام وصعبة التوجيه في أخرى، لذلك يُفضل تخصيصها لما تتفوق فيه واستخدام الكود التقليدي لبقية المهام. كقاعدة عامة:

* اعتمد على النماذج اللغوية (LLM) لاستخراج وفهم الصور والنصوص والإجابة على الأسئلة الإبداعية.
* استخدم الأكواد البرمجية العادية (Code) لعمليات إدارة البيانات، حفظ قوائم المهام (Task list)، والحلقات التكرارية المنتظمة (Loops)، وتجنب الاعتماد على الذكاء الاصطناعي فيها قدر الإمكان للحصول على أداء أسرع واستقرار برمجي.

## وضعية الطلب (Ask) مقابل الوكيل (Agent)

الفرق في طريقة تفاعل النماذج اللغوية يعتمد على الأدوات الممنوحة لها:

* **وضعية الـ Ask:** أن تعطي مطالبة (Prompt) للنموذج دون منحه أي أدوات تؤثر على العالم الحقيقي وتقرأ إجابته.
* **وضعية الـ Agent:** تزويد النموذج اللغوي بأدوات (Tools) تسمح له باتخاذ إجراءات (Actions) وقراءة ملفات وتنفيذ أوامر لتسلسل عمله نحو الحل.

## حواجز الحماية (Guardrails)

عند إعطاء الـ LLM أدوات ليتصرف كوكيل (Agent)، تقع على عاتقك مسؤولية توقع تصرفات غير مألوفة أو عشوائية، ولتجنب ذلك يجب:

1. كتابة سلسلة من الاختبارات لتقييم كيفية استغلال الـ LLM للأداة المتاحة (Mock testing).
2. إدراج العنصر البشري (Human-in-the-loop) لحل النزاعات. كمثال: عندما يحاول النظام وضع كلمة تتعارض مع حروف شبكة الكلمات المتقاطعة، بدلاً من تركه يقرر عشوائياً، يعرض التطبيق حواراً (Dialog) يطلب تدخل المستخدم للموافقة أو التعديل.
